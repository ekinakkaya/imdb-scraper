{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "class IMDBLinkScraper:\n",
    "    root_search_url = \"https://www.imdb.com/search/title/?\"\n",
    "\n",
    "    # we will add this to the link with the formatting: ?release_date=2024-10-03,2024-10-03\n",
    "    release_date_query = \"&release_date=\"\n",
    "\n",
    "    # the problem is that there are a lot of movies. for example there are 4,711 movies\n",
    "    # in the day of 01-01-2023 and 2,719,230 movies in the year 2023 alone\n",
    "    # so if we think about it, even if we only fetch the day 01-01-2023, there will be 4711 movies.\n",
    "    # and IMDb made the search page in a way that it only shows 50 of the movies initially\n",
    "    # and you have to click the \"Show More\" button to see 50 more movies. so we would need to click\n",
    "    # 4711 / 50 times which is 95 clicks. lets say if every click and fetch takes 5 seconds. 95 x 5 = 471 seconds.\n",
    "    # almost 8 minutes. for a day of movies. what? anyways we'll just test and see\n",
    "    \n",
    "    # what we will do is that we will first check every year individually and save the number of movies for that year\n",
    "    # then we will calculate how should we adjust the release date parameter\n",
    "\n",
    "    SCRAPING_MODES = [\"DAY\", \"MONTH\", \"YEAR\"]\n",
    "    # if the year has > 500.000 movies, scraping mode will be DAY\n",
    "    # if the year has > 50.000 movies, scraping mode will be MONTH\n",
    "    # if the year has > 5.000 movies, scraping mode will be YEAR\n",
    "\n",
    "    scraped_imdb_movie_links = []\n",
    "\n",
    "    yearly_counts_file_path = \"yearly_counts.json\"\n",
    "\n",
    "    DRIVER_INITIATED = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = Logger(\"imdb_link_scraper.log\").getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(\"starting IMDB Link scraper\")\n",
    "        # we dont do this because we dont want to open a new browser every time we run the script\n",
    "        # we want to use the same browser instance\n",
    "        #self.init_driver()\n",
    "\n",
    "    def init_driver(self):\n",
    "        if self.DRIVER_INITIATED:\n",
    "            return\n",
    "\n",
    "        self.logger.info(\"initializing IMDb link scraper\")\n",
    "\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.wait = WebDriverWait(self.driver, 5)\n",
    "        self.logger.info(\"initialized IMDb link scraper with \" + self.driver.name + \" driver.\")\n",
    "        self.DRIVER_INITIATED = True\n",
    "\n",
    "    def navigate(self, url):\n",
    "        self.init_driver()\n",
    "        self.driver.get(url)\n",
    "        self.logger.info(\"navigated to \" + url)\n",
    "\n",
    "    def create_search_query(self, start_date, end_date):\n",
    "        return self.root_search_url + self.release_date_query + start_date + \",\" + end_date\n",
    "\n",
    "    def read_yearly_movie_counts_file(self, filename):\n",
    "        data_normalized = {}\n",
    "\n",
    "        with open(filename, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            # we normalize the date while reading the file\n",
    "            data_normalized[\"updated_at\"] = datetime.datetime.strptime(data[\"updated_at\"], \"%Y-%m-%d\")\n",
    "            data_normalized[\"yearly_counts\"] = {}\n",
    "\n",
    "            for year in data[\"yearly_counts\"]:\n",
    "                data_normalized[\"yearly_counts\"][year] = data[\"yearly_counts\"][year]\n",
    "        \n",
    "        return data_normalized\n",
    "\n",
    "    def write_yearly_movie_counts_file(self, filename, updated_at: any, yearly_counts):\n",
    "        today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        data_structured = {\n",
    "            \"updated_at\": today,\n",
    "            \"yearly_counts\": yearly_counts\n",
    "        }\n",
    "\n",
    "        with open(filename, \"w\") as file:\n",
    "            self.logger.info(\"writing the yearly movie counts to JSON file: \" + filename + \" with indent=2. updated_at=\" + updated_at)\n",
    "            json.dump(data_structured, file, indent=2)\n",
    "\n",
    "    # from 1800 to this day, fetch how many movies are there in every year\n",
    "    # this is for understanding the data and how we should proceed fetching it\n",
    "    def fetch_yearly_movie_count(self, start_year: int, end_year: int):\n",
    "        self.init_driver()\n",
    "        # if a yearly_counts.json file exists and it is not older than 7 days, we just return the yearly_counts\n",
    "\n",
    "        if (os.path.exists(self.yearly_counts_file_path)):\n",
    "            self.logger.info(\"a yearly_counts.json file already exists. checking if it is viable\")\n",
    "\n",
    "            data = self.read_yearly_movie_counts_file(self.yearly_counts_file_path)\n",
    "\n",
    "            updated_at = data[\"updated_at\"] # this is already in datetime format\n",
    "            now = datetime.datetime.now()\n",
    "            seven_days_ago = now - datetime.timedelta(days=7)\n",
    "\n",
    "            if updated_at > seven_days_ago:\n",
    "                if str(start_year) in data[\"yearly_counts\"] and str(end_year) in data[\"yearly_counts\"]:\n",
    "                    self.logger.info(\"yearly_counts.json file is viable. returning the data\")\n",
    "                    return data\n",
    "                else:\n",
    "                    self.logger.warning(\"yearly_counts.json file does not contain the needed years. going to start fetching new data\")\n",
    "            else:\n",
    "                self.logger.info(\"yearly_counts.json file is older than 7 days. going to start fetching new data\")\n",
    "        else:\n",
    "            self.logger.info(\"yearly_counts.json file does not exist. going to start fetching it\")\n",
    "            \n",
    "                \n",
    "        yearly_counts: dict = {}\n",
    "        \n",
    "        for year in range(start_year, end_year + 1):\n",
    "            movie_count: int = 0\n",
    "            \n",
    "            start_date = str(year) + \"-01-01\"\n",
    "            end_date = str(int(year + 1)) + \"-01-01\"\n",
    "\n",
    "            self.logger.info(\"fetching movie count for interval: \" + start_date + \" | \" + end_date)\n",
    "\n",
    "            self.navigate(self.create_search_query(start_date, end_date))\n",
    "            self.driver.implicitly_wait(10)\n",
    "\n",
    "            movie_count_element_list = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[1]/div[1]\")\n",
    "            if (movie_count_element_list != []):\n",
    "                movie_count_text: str = movie_count[0].text\n",
    "                if movie_count_text == \"\":\n",
    "                    movie_count = 0\n",
    "                else:\n",
    "                    movie_count = int(movie_count.split(\"of \")[1].replace(\",\", \"\"))\n",
    "                \n",
    "                print(\"movie count for year \" + str(year) + \": \" + str(movie_count))\n",
    "                self.logger.info(\"found movie count for interval: \" + start_date + \" | \" + end_date + \" is \" + str(movie_count))\n",
    "            else:\n",
    "                movie_count = 0\n",
    "                print(\"movie count unknown for year \" + str(year))\n",
    "                self.logger.warning(\"movie count unknown for year \" + str(year))\n",
    "\n",
    "            yearly_counts[year] = movie_count\n",
    "\n",
    "        self.logger.info(\"finished fetching movie counts for years \" + str(start_year) + \" to \" + str(end_year))\n",
    "\n",
    "        self.write_yearly_movie_counts_file(self.yearly_counts_file_path, yearly_counts)\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "        \n",
    "    # TODO: make a yearly movie count class inside this class\n",
    "    \n",
    "    # TODO: make a method that creates a scrape path. a scrape path will be a list of dates for the program to scrape while scraping the actual movies.\n",
    "    # explanation: the yearly_counts.json file contains the yearly counts.\n",
    "    # if the yearly count is > 500000, the scraping mode will be DAY.\n",
    "    # and the program will create a series of days for scraping. like:\n",
    "    # [\"2024-01-01,2024-01-01\", \"2024-01-02,2024-01-02\", ...]\n",
    "    # if it was monthly, it wwould be like this:\n",
    "    # [\"2024-01-01,2024-01-31\", \"2024-02-01,2024-02-29\"]\n",
    "    \n",
    "    def create_scrape_path_from_yearly_counts(self, yearly_counts_filename=yearly_counts_file_path):\n",
    "        data = self.read_yearly_movie_counts_file(yearly_counts_filename)\n",
    "\n",
    "        scrape_path = []\n",
    "        \n",
    "        for year in data[\"yearly_counts\"]:\n",
    "            if data[\"yearly_counts\"][year] > 500000:\n",
    "                print(\"the year \" + year + \" should be scraped daily\")\n",
    "\n",
    "                start_date = datetime.datetime(int(year), 1, 1)\n",
    "                delta = datetime.timedelta(days=1)\n",
    "                end_date = datetime.datetime(int(year) + 1, 1, 1) - delta # last day of the year\n",
    "                \n",
    "                current_date = start_date\n",
    "                while current_date < end_date:\n",
    "                    next_date = current_date + delta\n",
    "                    day = current_date.strftime(\"%Y-%m-%d\")\n",
    "                    scrape_path.append(day + \",\" + day)\n",
    "                    current_date = next_date\n",
    "\n",
    "            elif data[\"yearly_counts\"][year] > 50000:\n",
    "                print(\"the year \" + year + \" should be scraped monthly\")\n",
    "\n",
    "                for month in range(1, 13):\n",
    "                    start_date = datetime.datetime(int(year), month, 1)\n",
    "\n",
    "                    if month == 12:\n",
    "                        end_date = datetime.datetime(int(year) + 1, 1, 1) - datetime.timedelta(days=1)\n",
    "                    else:\n",
    "                        end_date = datetime.datetime(int(year), month + 1, 1) - datetime.timedelta(days=1)\n",
    "\n",
    "                    scrape_path.append(start_date.strftime(\"%Y-%m-%d\") + \",\" + end_date.strftime(\"%Y-%m-%d\"))\n",
    "            elif data[\"yearly_counts\"][year] > 0:\n",
    "                print(\"the year \" + year + \" should be scraped yearly\")\n",
    "                scrape_path.append(year + \"-01-01,\" + year + \"-12-31\")\n",
    "            else:\n",
    "                print(\"the year \" + year + \" should not be scraped\")\n",
    "\n",
    "        self.write_scrape_path_to_json(scrape_path)\n",
    "\n",
    "        self.logger.info(\"created scrape path from yearly counts\")\n",
    "        \n",
    "        return scrape_path\n",
    "\n",
    "    def write_scrape_path_to_json(self, scrape_path):\n",
    "        data_structured = {\n",
    "            \"scrape_path_total_count\": len(scrape_path),\n",
    "            \"scrape_path_entries\": {\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for i in range(len(scrape_path)):\n",
    "            data_structured[\"scrape_path_entries\"][i] = scrape_path[i]\n",
    "\n",
    "        with open(\"scrape_path.json\", \"w\") as file:\n",
    "            self.logger.info(\"writing the scrape path to JSON file: scrape_path.json\")\n",
    "            json.dump(data_structured, file, indent=2)\n",
    "\n",
    "    def read_scrape_path_from_file(self, filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            scrape_path = []\n",
    "            \n",
    "            for i in range(data[\"scrape_path_total_count\"]):\n",
    "                scrape_path.append(data[\"scrape_path_entries\"][str(i)])\n",
    "            return scrape_path\n",
    "\n",
    "        return []\n",
    "        \n",
    "\n",
    "    # we close the preferences popup by clicking the decline button\n",
    "    # has this xpath\n",
    "    # //*[@id=\"__next\"]/div/div/div[2]/div/button[1]\n",
    "    def close_bad_imdb_popup(self):\n",
    "        self.logger.info(\"closing the bad IMDB popup if exists\")\n",
    "        decline_button = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/div/div/div[2]/div/button[1]\")\n",
    "\n",
    "        if len(decline_button) > 0:\n",
    "            decline_button[0].click()\n",
    "            self.logger.info(\"closed the bad imdb preferences popup, waiting for 2 secs\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        \n",
    "                \n",
    "    def scrape_movies_from_scrape_path(self, scrape_path_filename):\n",
    "        scrape_path = self.read_scrape_path_from_file(scrape_path_filename)\n",
    "        self.init_driver()\n",
    "\n",
    "        for interval in scrape_path:\n",
    "            self.logger.info(\"starting scraping movies from time interval: \" + interval)\n",
    "            start_date, end_date = interval.split(\",\")\n",
    "            self.navigate(self.create_search_query(start_date, end_date))\n",
    "            self.driver.implicitly_wait(5)\n",
    "            time.sleep(3)\n",
    "\n",
    "            self.close_bad_imdb_popup()\n",
    "\n",
    "            movie_count: int = 0\n",
    "            \n",
    "            movie_count_element_list = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[1]/div[1]\")\n",
    "            if (movie_count_element_list != []):\n",
    "                movie_count_text: str = movie_count_element_list[0].text\n",
    "                if movie_count_text == \"\":\n",
    "                    movie_count = 0\n",
    "                else:\n",
    "                    movie_count = int(movie_count_text.split(\"of \")[1].replace(\",\", \"\"))\n",
    "                \n",
    "                print(\"movie count for interval \" + start_date + \", \" + end_date + \": \" + str(movie_count))\n",
    "                self.logger.info(\"movie count for interval: \" + start_date + \" | \" + end_date + \" is \" + str(movie_count))\n",
    "\n",
    "                clicks_needed = int(movie_count / 50) + 1\n",
    "                self.logger.info(\"we will click \" + str(clicks_needed) + \" times to get all the movies\")\n",
    "\n",
    "                # we click the see more button until it disappears\n",
    "                count_fifty_more_clicked = 0\n",
    "                while True:\n",
    "                    # we check if \n",
    "                    more_button_list = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[2]/div/span/button\")\n",
    "                    if len(more_button_list) == 0:\n",
    "                        self.logger.info(\"could not found the 50 more button. we probably have all the movies now.\")\n",
    "\n",
    "                        # find all the movies in the page and get their links\n",
    "                        # the xpath of a movie link:\n",
    "                        # //*[@id=\"__next\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/ul/li[1]/div/div/div/div[1]/div[2]/div[1]/a\n",
    "                        self.logger.info(\"finding all the movie links\")\n",
    "                        movies = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/ul/li/div/div/div/div[1]/div[2]/div[1]/a\")\n",
    "\n",
    "                        # write to a file\n",
    "                        with open(str(start_date) + \"_\" + str(end_date) + \"_\" + datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\") + \".imdb_scraper.txt\", \"a\") as file:\n",
    "                            for movie in movies:\n",
    "                                # remove the ?ref_=sr_t_1 stuff\n",
    "                                movie_link = movie.get_attribute(\"href\").split(\"?\")[0]\n",
    "                                file.write(movie_link + \"\\n\")\n",
    "\n",
    "                        break\n",
    "                    else:\n",
    "                        self.logger.info(\"found the '50 more' button. scrolling to it.\")\n",
    "\n",
    "\n",
    "                        element_position = self.driver.execute_script(\"return arguments[0].getBoundingClientRect().top;\", more_button_list[0])\n",
    "                        self.driver.execute_script(\"window.scrollBy(0, arguments[0] - 200);\", element_position)\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        self.logger.info(\"clicking the '50 more' button\")\n",
    "\n",
    "                        #more_button_list[0].click()\n",
    "\n",
    "                        # we should try scrolling and clicking in a loop until it succeeds with try catch\n",
    "                        while True:\n",
    "                            try:\n",
    "                                # relocate the button before clicking\n",
    "                                more_button_list = self.driver.find_elements(by=By.XPATH, value=\"//*[@id=\\\"__next\\\"]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[2]/div/span/button\")\n",
    "                                if more_button_list == []:\n",
    "                                    self.logger.info(\"could not found the 50 more button. we probably have all the movies now.\")\n",
    "                                    break\n",
    "                                more_button_list[0].click()\n",
    "                                time.sleep(1)\n",
    "                                self.driver.implicitly_wait(2)\n",
    "                                break\n",
    "                            except:\n",
    "                                self.logger.info(\"could not click the '50 more' button. scrolling to it.\")\n",
    "                                element_position = self.driver.execute_script(\"return arguments[0].getBoundingClientRect().top;\", more_button_list[0])\n",
    "                                self.driver.execute_script(\"window.scrollBy(0, arguments[0] - 200);\", element_position)\n",
    "                                time.sleep(2)\n",
    "                        \n",
    "                        count_fifty_more_clicked += 1\n",
    "                        self.logger.info(\"clicked \" + str(count_fifty_more_clicked) + \" times. [\" + str(count_fifty_more_clicked) + \"/\" + str(clicks_needed) + \"] | {\" + start_date + \", \" + end_date + \"}\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                movie_count = 0\n",
    "                print(\"movie count unknown for interval \" + start_date + \", \" + end_date)\n",
    "                self.logger.warning(\"movie count unknown for interval \" + start_date + \", \" + end_date)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie count for interval 1900-01-01, 1900-03-31: 119\n"
     ]
    }
   ],
   "source": [
    "scraper = IMDBLinkScraper()\n",
    "#yearly_counts_data = scraper.fetch_yearly_movie_count(2000, 2024)\n",
    "#scraper.create_scrape_path_from_yearly_counts(\"yearly_counts.json\")\n",
    "scraper.scrape_movies_from_scrape_path(\"scrape_path.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
